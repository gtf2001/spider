# 9.21

## 笔趣阁爬虫

### 需要的包

```python
from urllib.request import Request
from urllib.request import urlopen
import gzip
from lxml import etree
```

### 知识点

1.  [urllib.request模块中的Request类的使用](#jump1)
2.  [urllib.request模块的urlopen方法](#jump2)
3.  [gzip模块的解压缩depressed](#jump3)
4.  [lxml第三方库中的etree类的使用](#jump4)

####  <span id="jump1">1. urllib.request模块中的Request类的使用</span>

内置模块urllib

爬虫的核心是请求与响应

Request类用于打开一个请求。是写爬虫的第一步

**用法：**

变量名 = Request(url="url(str类型)"[, headers = {header(dict字典)}])  

**举例：**

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/83.0.4103.61 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,"
              "application/signed-exchange;v=b3;q=0.9"}
r = Request(url="https://www.xbiquge.la/xiaoshuodaquan/", headers=headers)
```



#### <span id="jump2">2.  urllib.request模块的urlopen方法</span>

内置模块urllib

[urllib](https://so.csdn.net/so/search?q=urllib&spm=1001.2101.3001.7020).request.urlopen()函数用于实现对目标url的访问。

 

函数原型如下：urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)　

url: 需要打开的网址

data：Post提交的数据

timeout：设置网站的访问超时时间

 

直接用urllib.request模块的urlopen（）获取页面，page的数据格式为bytes类型，需要decode（）解码，转换成str类型。

**用法：**

response = urlopen(Requset对象)

**举例：**

```python
response = ur.urlopen(r)
print(response.read().decode('utf-8'))
```

**返回处理方法详解**

urlopen返回的是**HTTPResposne类型**的对象。

urlopen返回对象提供方法：

read() , readline() ,readlines() , fileno() , close() ：对HTTPResponse类型数据进行操作

info()：返回HTTPMessage对象，表示远程服务器返回的头信息

getcode()：返回Http状态码。如果是http请求，200请求成功完成;404网址未找到

geturl()：返回请求的url



#### 3. <span id="jump3">gzip模块的解压缩depressed</span>

gzip模块能够直接压缩和解压缩bytes-like类型的数据，同时也能实现对应格式文件的压缩与解压缩.

- ### 压缩

  - ### **gzip.compress(data)**

    - data：需要压缩的bytes-like类型数据
    - compresslevel参数：可选，用数字0-9表示压缩级别，默认最高压缩级别9，0表示不压缩

- ### 解压缩

  - ### **gzip.decompress(data)**



#### <span id='jump4'>4. lxml第三方库中的etree类的使用</span>

+ **lxml模块的安装**

  安装方式：在终端[cmd](https://so.csdn.net/so/search?q=cmd&spm=1001.2101.3001.7020)下利用pip命令安装即可（保证网络畅通）

  ```
  pip install lxml
  ```

+ **element对象**

  element对象是xpath语法的使用对象，element对象可由html[字符串](https://so.csdn.net/so/search?q=字符串&spm=1001.2101.3001.7020)转化

  - 利用etree.HTML()将html字符串转化为element对象 ,

    ```python
    from lxml import etree
    MyStr = '''<meta name="baidu-site-verification" content="cZdR4xxR7RxmM4zE" />
        <meta http-equiv="Pragma" content="no-cache">
        <meta http-equiv="Expires" content="Sun, 6 Mar 2005 01:00:00 GMT">
        '''
    HtmlElement = etree.HTML(MyStr) 
    print(type(HtmlElement))
    #<class 'lxml.etree._Element'>
    ```

  - 将element对象转化为字符串

    etree的tostring方法可以将element转化为二进制类型。故**需要用encoding属性指定编码方法**，否则可能会造成乱码。

    **且此方法会使原来不规则的html字符串补全为规则的html**

    ```python
    HtmlStr=etree.tostring(HtmlElement,encoding="utf-8").decode()
    print(HtmlStr)
    ```

    输出：

    ```xml
    <html><head><meta name="baidu-site-verification" content="cZdR4xxR7RxmM4zE"/>
        <meta http-equiv="Pragma" content="no-cache"/>
        <meta http-equiv="Expires" content="Sun, 6 Mar 2005 01:00:00 GMT"/>
        </head></html>
    ```

+ **element对象的xpath方法**

  利用etree.HTML，将字符串转化为Element对象,Element对象具有xpath的方法,返回结果的列表，能够接受bytes类型的数据和str类型的数据

  ```python
  html = etree.HTML(text)  
  ret_list = html.xpath("xpath字符串")
  ```

  **lxml的高阶使用：**当提取标签的多种属性时，可以分组提取相应的标签，在对每个标签进行处理即可，这样可防止有些标签没有相应的属性，导致信息列表对应错误

