# Scrapy

- 引言

~~~markdown
1. 是一个爬虫框架
	requests第三方库
~~~

## Requests的问题

~~~markdown
框架和库：
	1. 用法简单，功能单一
	2. 用法复杂，功能强大
	
1. 效率慢
2. 不能对数据进行调度（对url进行去重、容错）
3. 多次请求与响应代码会冗余
4. 数据库代码冗余
~~~

![爬虫框架](G:\太原理工2019爬虫\笔记\.assets\爬虫框架.png)

# Scrapy

- 引言

~~~markdown
1. 是一个爬虫框架
	requests第三方库
2. 是一个专业的爬虫框架，完成了多线程爬取
	框架是代码的半成品
3. 可以和其他的外部组件一起使用，实现特殊操作
	scrapy-redis
~~~

-  scrapy的安装

~~~markdown
1. 是第三方框架
		pip install scrapy
2. 需要依赖
		1. vc++库
		2. win32api:pip install pywin32
		
pip install scrapy --no-binary
~~~

## Scrapy的架构

![scrapy架构](G:\太原理工2019爬虫\笔记\.assets\scrapy架构.png)



### 组件的作用

~~~markdown
1. 引擎：负责各个组件之间的数据交互，以及代码的运行流程。
2. Scheduler（调度器）：接收Requests对象，将Requests对象，给Downloader
3. Downloader(下载器)：从Scheduler接收Requests对象，发出Requests对象，接收Response
4. Spiders(解析器)：如果response解析后是数据的话，则封装成Items对象传递给Item Pipeline，如果是url，则封装成Request对象传递给调度器。
5. pipeline(管道)：存储数据
6. 中间件：解耦合
~~~

## Scrapy的工作原理

~~~markdown
1. 从Spider中获取起始url，封装成Request对象，交给引擎，引擎交给调度器
2. 调度器获取到一个Request并保存起来，将存的Request对象进行调度后发送给引擎，引擎发送给下载器
3. 下载器将接受到的Request对象发送给互联网，获得Response对象，将Response对象发送给引擎，引擎发送给spider
4. Spider对接收到的Response对象进行解析，如果解析出来的是数据，则封装成Items发送给引擎，引擎发送给Pipeline;如果解析出来的是ur诶楼，则封装成Request对象，发送给引擎，引擎发送给调度器。
5. Pipeline拿到数据后存储在数据库中
6. 一直重复2-6部即可
~~~

### 创建项目

~~~markdown
1. 创建项目
		在terminal里面，进入到项目文件夹中，输入
		scrapy startproject 项目名
2. 创建spider爬虫
		scrapy genspider 爬虫名 入口地址
3. 运行爬虫
		scrapy crawl 爬虫名
4. 技巧：
		1. 在爬虫的根目录(scrapy.cfg)中创建一个main.py的文件
		2. 利用操作系统发送指令
~~~

~~~python
from scrapy import cmdline

cmdline.execute(['scrapy','crawl','sp1'])
~~~

### 作业

~~~markdown
1. 把之前的所有爬虫，全部改成scrapy
2. 其中前程无忧，需要获取到500W数据
3. 完成图像审核（输入一个图片，输出该图片是否合规，如果不合规的话，请说明，图像中包含色情、政治敏感、血腥暴力等原因）
~~~

























